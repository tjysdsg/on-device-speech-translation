@inproceedings{espnet,
 author = {Shinji Watanabe and
Takaaki Hori and
Shigeki Karita and
Tomoki Hayashi and
Jiro Nishitoba and
Yuya Unno and
Nelson Enrique Yalta Soplin and
Jahn Heymann and
Matthew Wiesner and
Nanxin Chen and
Adithya Renduchintala and
Tsubasa Ochiai},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/interspeech/WatanabeHKHNUSH18.bib},
 booktitle = {Interspeech 2018, 19th Annual Conference of the International Speech
Communication Association, Hyderabad, India, 2-6 September 2018},
 doi = {10.21437/Interspeech.2018-1456},
 pages = {2207--2211},
 publisher = {{ISCA}},
 timestamp = {Fri, 29 Jan 2021 00:00:00 +0100},
 title = {ESPnet: End-to-End Speech Processing Toolkit},
 url = {https://doi.org/10.21437/Interspeech.2018-1456},
 year = {2018}
}

@article{see2016compression,
  title={Compression of neural machine translation models via pruning},
  author={See, Abigail and Luong, Minh-Thang and Manning, Christopher D},
  journal={arXiv preprint arXiv:1606.09274},
  year={2016}
}
@article{xu2023recent,
  title={Recent Advances in Direct Speech-to-text Translation},
  author={Xu, Chen and Ye, Rong and Dong, Qianqian and Zhao, Chengqi and Ko, Tom and Wang, Mingxuan and Xiao, Tong and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2306.11646},
  year={2023}
}
@article{inaguma2020espnet,
  title={ESPnet-ST: All-in-one speech translation toolkit},
  author={Inaguma, Hirofumi and Kiyono, Shun and Duh, Kevin and Karita, Shigeki and Soplin, Nelson Enrique Yalta and Hayashi, Tomoki and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2004.10234},
  year={2020}
}
@incollection{di2019adapting,
  title={Adapting transformer to end-to-end spoken language translation},
  author={Di Gangi, Mattia A and Negri, Matteo and Turchi, Marco},
  booktitle={Proceedings of INTERSPEECH 2019},
  pages={1133--1137},
  year={2019},
  publisher={International Speech Communication Association (ISCA)}
}
@article{watanabe2017hybrid,
  title={Hybrid CTC/attention architecture for end-to-end speech recognition},
  author={Watanabe, Shinji and Hori, Takaaki and Kim, Suyoun and Hershey, John R and Hayashi, Tomoki},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={11},
  number={8},
  pages={1240--1253},
  year={2017},
  publisher={IEEE}
}

@inproceedings{CVSS,
  title={CVSS Corpus and Massively Multilingual Speech-to-Speech Translation},
  author={Yeting Jia and Michelle Tadmor Ramanovich and Quan Wang and Heiga Zen},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:245853756}
}

@inproceedings{conformer,
 author = {Anmol Gulati and
James Qin and
Chung{-}Cheng Chiu and
Niki Parmar and
Yu Zhang and
Jiahui Yu and
Wei Han and
Shibo Wang and
Zhengdong Zhang and
Yonghui Wu and
Ruoming Pang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/interspeech/GulatiQCPZYHWZW20.bib},
 booktitle = {Interspeech 2020, 21st Annual Conference of the International Speech
Communication Association, Virtual Event, Shanghai, China, 25-29 October
2020},
 doi = {10.21437/Interspeech.2020-3015},
 pages = {5036--5040},
 publisher = {{ISCA}},
 timestamp = {Fri, 29 Jan 2021 00:00:00 +0100},
 title = {Conformer: Convolution-augmented Transformer for Speech Recognition},
 url = {https://doi.org/10.21437/Interspeech.2020-3015},
 year = {2020}
}

@inproceedings{transformer,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 pages = {5998--6008},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{CTC,
author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143891},
doi = {10.1145/1143844.1143891},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {369–376}
}

@inproceedings{mustc,
    title = "{M}u{ST}-{C}: a {M}ultilingual {S}peech {T}ranslation {C}orpus",
    author = "Di Gangi, Mattia A.  and
      Cattoni, Roldano  and
      Bentivogli, Luisa  and
      Negri, Matteo  and
      Turchi, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1202",
    doi = "10.18653/v1/N19-1202",
    pages = "2012--2017",
    abstract = "Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction.",
}


@inproceedings{deepspeed,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {machine learning, distributed deep learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}


@inproceedings{bleu,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

  